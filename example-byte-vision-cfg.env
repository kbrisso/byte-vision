
###Global app folder, path settings
AppLogPath=C:/Projects/byte-vision/logs/
AppLogFileName=byte-vision.log
PromptTempPath=C:/Projects/byte-vision/prompt-temp/
PromptCachePath=C:/Projects/byte-vision/prompt-cache/
ModelPath=C:/Projects/byte-vision/models/
LLamaCliPath=C:/Projects/byte-vision/llamacpp/llama-cli.exe
LLamaEmbedCliPath=C:/Projects/byte-vision/llamacpp/llama-embedding.exe
DocumentPath=C:/Projects/byte-vision/document/
PDFToTextPath=C:/Projects/byte-vision/xpdf-tools/bin64/pdftotext.exe
PDFToImagesPath=C:/Projects/byte-vision/xpdf-tools/bin64/pdfimages.exe
ModelFileName=Qwen3-8B-Q8_0.gguf
EmbedModelFileName=mxbai-embed-large-v1-f16.gguf
ModelLogPath=C:/Projects/byte-vision/logs/
MongoURI=mongodb://localhost:27017/
TesseractPath=C:/Program Files/Tesseract-OCR/tesseract.exe
ElasticsearchAPIKey = ZmpuM2JwWUJ1MVdQTjdYZTdvejA6NmtXT3R1NFR0b3hBWFlGS1I0N0xRZw==
ElasticsearchServerAddresses = http://localhost:9200

###Default llama-embedding settings - Reordered to match help output###
Description=Default

# --completion-bash - print source-able bash completion script for llama.cpp
EmbedCompletionBashCmd=--completion-bash
EmbedCompletionBashCmdEnabled=false

# --verbose-prompt - print a verbose prompt before generation (default: false)
EmbedVerbosePromptCmd=--verbose-prompt
EmbedVerbosePromptCmdEnabled=false

# -t, --threads N - number of threads to use during generation (default: -1)
EmbedThreadsCmd=--threads
EmbedThreadsVal=8

# -tb, --threads-batch N - number of threads to use during batch and prompt processing (default: same as --threads)
EmbedThreadsBatchCmd=--threads-batch
EmbedThreadsBatchVal=8

# -C, --cpu-mask M - CPU affinity mask: arbitrarily long hex. Complements cpu-range (default: "")
EmbedCpuMaskCmd=--cpu-mask
EmbedCpuMaskVal=

# -Cr, --cpu-range lo-hi - range of CPUs for affinity. Complements --cpu-mask
EmbedCpuRangeCmd=--cpu-range
EmbedCpuRangeVal=

# --cpu-strict <0|1> - use strict CPU placement (default: 0)
EmbedCpuStrictCmd=--cpu-strict
EmbedCpuStrictVal=

# --prio N - set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime (default: 0)
EmbedPrioCmd=--prio
EmbedPrioVal=

# --poll <0...100> - use polling level to wait for work (0 - no polling, default: 50)
EmbedPollCmd=--poll
EmbedPollVal=

# -Cb, --cpu-mask-batch M - CPU affinity mask: arbitrarily long hex. Complements cpu-range-batch (default: same as --cpu-mask)
EmbedCpuMaskBatchCmd=--cpu-mask-batch
EmbedCpuMaskBatchVal=

# -Crb, --cpu-range-batch lo-hi - ranges of CPUs for affinity. Complements --cpu-mask-batch
EmbedCpuRangeBatchCmd=--cpu-range-batch
EmbedCpuRangeBatchVal=

# --cpu-strict-batch <0|1> - use strict CPU placement (default: same as --cpu-strict)
EmbedCpuStrictBatchCmd=--cpu-strict-batch
EmbedCpuStrictBatchVal=

# --prio-batch N - set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime (default: 0)
EmbedPrioBatchCmd=--prio-batch
EmbedPrioBatchVal=

# --poll-batch <0|1> - use polling to wait for work (default: same as --poll)
EmbedPollBatchCmd=--poll-batch
EmbedPollBatchVal=

# -c, --ctx-size N - size of the prompt context (default: 4096, 0 = loaded from model)
EmbedCtxSizeCmd=--ctx-size
EmbedCtxtSizeVal=512

# -n, --predict, --n-predict N - number of tokens to predict (default: -1, -1 = infinity)
EmbedPredictCmd=--n-predict
EmbedPredictVal=

# -b, --batch-size N - logical maximum batch size (default: 2048)
EmbedBatchCmd=--batch-size
EmbedBatchVal=512

# -ub, --ubatch-size N - physical maximum batch size (default: 512)
EmbedUBatchCmd=--ubatch-size
EmbedUBatchVal=

# --keep N - number of tokens to keep from the initial prompt (default: 0, -1 = all)
EmbedKeepCmd=--keep
EmbedKeepVal=0

# -fa, --flash-attn - enable Flash Attention (default: disabled)
EmbedFlashAttentionCmd=--flash-attn
EmbedFlashAttentionCmdEnabled=true

# -p, --prompt PROMPT - prompt to start generation with; for system message, use -sys
EmbedPromptCmd=-p
EmbedPromptCmdEnabled=false
EmbedPromptText=

# --no-perf - disable internal libllama performance timings (default: false)
EmbedNoPerfCmd=--no-perf
EmbedNoPerfCmdEnabled=false

# -f, --file FNAME - a file containing the prompt (default: none)
EmbedPromptFileCmd=--file
EmbedPromptFileVal=

# -bf, --binary-file FNAME - binary file containing the prompt (default: none)
EmbedBinaryFileCmd=--binary-file
EmbedBinaryFileVal=

# -e, --escape - process escapes sequences (\n, \r, \t, \', \", \\) (default: true)
EmbedEscapeNewLinesCmd=-e
EmbedEscapeNewLinesCmdEnabled=false

# --no-escape - do not process escape sequences
EmbedNoEscapeCmd=--no-escape
EmbedNoEscapeCmdEnabled=false

# --rope-scaling {none,linear,yarn} - RoPE frequency scaling method, defaults to linear unless specified by the model
EmbedRopeScalingCmd=--rope-scaling
EmbedRopeScalingVal=

# --rope-scale N - RoPE context scaling factor, expands context by a factor of N
EmbedRopeScaleCmd=--rope-scale
EmbedRopeScaleVal=

# --rope-freq-base N - RoPE base frequency, used by NTK-aware scaling (default: loaded from model)
EmbedRopeFreqBaseCmd=--rope-freq-base
EmbedRopeFreqBaseVal=

# --rope-freq-scale N - RoPE frequency scaling factor, expands context by a factor of 1/N
EmbedRopeFreqScaleCmd=--rope-freq-scale
EmbedRopeFreqScaleVal=

# --yarn-orig-ctx N - YaRN: original context size of model (default: 0 = model training context size)
EmbedYarnOrigContextCmd=--yarn-orig-ctx
EmbedYarnOrigContextVal=

# --yarn-ext-factor N - YaRN: extrapolation mix factor (default: -1.0, 0.0 = full interpolation)
EmbedYarnExtFactorCmd=--yarn-ext-factor
EmbedYarnExtFactorVal=

# --yarn-attn-factor N - YaRN: scale sqrt(t) or attention magnitude (default: 1.0)
EmbedYarnAttnFactorCmd=--yarn-attn-factor
EmbedYarnAttnFactorVal=

# --yarn-beta-slow N - YaRN: high correction dim or alpha (default: 1.0)
EmbedYarnBetaSlowCmd=--yarn-beta-slow
EmbedYarnBetaSlowVal=

# --yarn-beta-fast N - YaRN: low correction dim or beta (default: 32.0)
EmbedYarnBetaFastCmd=--yarn-beta-fast
EmbedYarnBetaFastVal=

# -dkvc, --dump-kv-cache - verbose print of the KV cache
EmbedDumpKvCacheCmd=--dump-kv-cache
EmbedDumpKvCacheCmdEnabled=false

# -nkvo, --no-kv-offload - disable KV offload
EmbedNoKvOffloadCmd=--no-kv-offload
EmbedNoKvOffloadCmdEnabled=false

# -ctk, --cache-type-k TYPE - KV cache data type for K (default: f16)
EmbedCacheTypeKCmd=--cache-type-k
EmbedCacheTypeKVal=

# -ctv, --cache-type-v TYPE - KV cache data type for V (default: f16)
EmbedCacheTypeVCmd=--cache-type-v
EmbedCacheTypeVVal=

# -dt, --defrag-thold N - KV cache defragmentation threshold (default: 0.1, < 0 - disabled)
EmbedDefragTholdCmd=--defrag-thold
EmbedDefragTholdVal=

# -np, --parallel N - number of parallel sequences to decode (default: 1)
EmbedParallelCmd=--parallel
EmbedParallelVal=

# --rpc SERVERS - comma separated list of RPC servers
EmbedRpcCmd=--rpc
EmbedRpcVal=

# --mlock - force system to keep model in RAM rather than swapping or compressing
EmbedMemLockCmd=--mlock
EmbedMemLockCmdEnabled=false

# --no-mmap - do not memory-map model (slower load but may reduce pageouts if not using mlock)
EmbedNoMmapCmd=--no-mmap
EmbedNoMmapCmdEnabled=false

# --numa TYPE - attempt optimizations that help on some NUMA systems
EmbedNumaCmd=--numa
EmbedNumaVal=

# -dev, --device <dev1,dev2,..> - comma-separated list of devices to use for offloading
EmbedDeviceCmd=--device
EmbedDeviceVal=

# --list-devices - print list of available devices and exit
EmbedListDevicesCmd=--list-devices
EmbedListDevicesCmdEnabled=false

# --override-tensor, -ot <tensor name pattern>=<buffer type>,... - override tensor buffer type
EmbedOverrideTensorCmd=--override-tensor
EmbedOverrideTensorVal=

# -ngl, --gpu-layers, --n-gpu-layers N - number of layers to store in VRAM
EmbedGPULayersCmd=--n-gpu-layers
EmbedGPULayersVal=33

# -sm, --split-mode {none,layer,row} - how to split the model across multiple GPUs
EmbedSplitModeCmd=--split-mode
EmbedSplitModeVal=

# -ts, --tensor-split N0,N1,N2,... - fraction of the model to offload to each GPU
EmbedTensorSplitCmd=--tensor-split
EmbedTensorSplitVal=

# -mg, --main-gpu INDEX - the GPU to use for the model (default: 0)
EmbedMainGPUCmd=--main-gpu
EmbedMainGPUVal=1

# --check-tensors - check model tensor data for invalid values (default: false)
EmbedCheckTensorsCmd=--check-tensors
EmbedCheckTensorsCmdEnabled=false

# --override-kv KEY=TYPE:VALUE - advanced option to override model metadata by key
EmbedOverrideKvCmd=--override-kv
EmbedOverrideKvVal=

# --lora FNAME - path to LoRA adapter (can be repeated to use multiple adapters)
EmbedLoraCmd=--lora
EmbedLoraVal=

# --lora-scaled FNAME SCALE - path to LoRA adapter with user defined scaling
EmbedLoraScaledCmd=--lora-scaled
EmbedLoraScaledVal=

# --control-vector FNAME - add a control vector
EmbedControlVectorCmd=--control-vector
EmbedControlVectorVal=

# --control-vector-scaled FNAME SCALE - add a control vector with user defined scaling SCALE
EmbedControlVectorScaledCmd=--control-vector-scaled
EmbedControlVectorScaledVal=

# --control-vector-layer-range START END - layer range to apply the control vector(s) to
EmbedControlVectorLayerRangeCmd=--control-vector-layer-range
EmbedControlVectorLayerRangeVal=

# -m, --model FNAME - model path
EmbedModelCmd=-m
EmbedModelFullPathVal=C:/Projects/byte-vision/models/mxbai-embed-large-v1-f16.gguf

# -mu, --model-url MODEL_URL - model download url (default: unused)
EmbedModelUrlCmd=--model-url
EmbedModelUrlVal=

# -hf, -hfr, --hf-repo <user>/<model>[:quant] - Hugging Face model repository
EmbedHfRepoCmd=--hf-repo
EmbedHfRepoVal=

# -hfd, -hfrd, --hf-repo-draft <user>/<model>[:quant] - Same as --hf-repo, but for the draft model
EmbedHfRepoDraftCmd=--hf-repo-draft
EmbedHfRepoDraftVal=

# -hff, --hf-file FILE - Hugging Face model file
EmbedHfFileCmd=--hf-file
EmbedHfFileVal=

# -hfv, -hfrv, --hf-repo-v <user>/<model>[:quant] - Hugging Face model repository for the vocoder model
EmbedHfRepoVCmd=--hf-repo-v
EmbedHfRepoVVal=

# -hffv, --hf-file-v FILE - Hugging Face model file for the vocoder model
EmbedHfFileVCmd=--hf-file-v
EmbedHfFileVVal=

# -hft, --hf-token TOKEN - Hugging Face access token
EmbedHfTokenCmd=--hf-token
EmbedHfTokenVal=

# --log-disable - Log disable
EmbedLogDisableCmd=--log-disable
EmbedLogDisableCmdEnabled=false

# --log-file FNAME - Log to file
EmbedModelLogFileCmd=--log-file
EmbedModelLogFileNameVal=C:/Projects/byte-vision/logs/mxbai-embed-large-v1-f16.log

# -v, --verbose, --log-verbose - Set verbosity level to infinity
EmbedLogVerboseCmd=--log-verbose
EmbedLogVerboseEnabled=false

# -lv, --verbosity, --log-verbosity N - Set the verbosity threshold
EmbedLogVerbosityCmd=--log-verbosity
EmbedLogVerbosityVal=

# --log-prefix - Enable prefix in log messages
EmbedLogPrefixCmd=--log-prefix
EmbedLogPrefixCmdEnabled=false

# --log-timestamps - Enable timestamps in log messages
EmbedLogTimestampsCmd=--log-timestamps
EmbedLogTimestampsCmdEnabled=false

# ----- sampling params -----

# --samplers SAMPLERS - samplers that will be used for generation in the order
EmbedSamplersCmd=--samplers
EmbedSamplersVal=

# -s, --seed SEED - RNG seed (default: -1, use random seed for -1)
EmbedRandomSeedCmd=--seed
EmbedRandomSeedVal=

# --sampling-seq, --sampler-seq SEQUENCE - simplified sequence for samplers that will be used
EmbedSamplingSeqCmd=--sampling-seq
EmbedSamplingSeqVal=

# --ignore-eos - ignore end of stream token and continue generating
EmbedIgnoreEosCmd=--ignore-eos
EmbedIgnoreEosCmdEnabled=false

# --temp N - temperature (default: 0.8)
EmbedTemperatureCmd=--temp
EmbedTemperatureVal=.5

# --top-k N - top-k sampling (default: 40, 0 = disabled)
EmbedTopKCmd=--top-k
EmbedTopKVal=20

# --top-p N - top-p sampling (default: 0.9, 1.0 = disabled)
EmbedTopPCmd=--top-p
EmbedTopPVal=0.9

# --min-p N - min-p sampling (default: 0.1, 0.0 = disabled)
EmbedMinPCmd=--min-p
EmbedMinPVal=0.1

# --xtc-probability N - xtc probability (default: 0.0, 0.0 = disabled)
EmbedXtcProbabilityCmd=--xtc-probability
EmbedXtcProbabilityVal=

# --xtc-threshold N - xtc threshold (default: 0.1, 1.0 = disabled)
EmbedXtcThresholdCmd=--xtc-threshold
EmbedXtcThresholdVal=

# --typical N - locally typical sampling, parameter p (default: 1.0, 1.0 = disabled)
EmbedTypicalCmd=--typical
EmbedTypicalVal=

# --repeat-last-n N - last n tokens to consider for penalize (default: 64, 0 = disabled, -1 = ctx_size)
EmbedRepeatLastPenaltyCmd=--repeat-last-n
EmbedRepeatLastPenaltyVal=

# --repeat-penalty N - penalize repeat sequence of tokens (default: 1.0, 1.0 = disabled)
EmbedRepeatPenaltyCmd=--repeat-penalty
EmbedRepeatPenaltyVal=

# --presence-penalty N - repeat alpha presence penalty (default: 0.0, 0.0 = disabled)
EmbedPresencePenaltyCmd=--presence-penalty
EmbedPresencePenaltyVal=

# --frequency-penalty N - repeat alpha frequency penalty (default: 0.0, 0.0 = disabled)
EmbedFrequencyPenaltyCmd=--frequency-penalty
EmbedFrequencyPenaltyVal=

# --dry-multiplier N - set DRY sampling multiplier (default: 0.0, 0.0 = disabled)
EmbedDryMultiplierCmd=--dry-multiplier
EmbedDryMultiplierVal=

# --dry-base N - set DRY sampling base value (default: 1.75)
EmbedDryBaseCmd=--dry-base
EmbedDryBaseVal=

# --dry-allowed-length N - set allowed length for DRY sampling (default: 2)
EmbedDryAllowedLengthCmd=--dry-allowed-length
EmbedDryAllowedLengthVal=

# --dry-penalty-last-n N - set DRY penalty for the last n tokens (default: -1, 0 = disable, -1 = context size)
EmbedDryPenaltyLastNCmd=--dry-penalty-last-n
EmbedDryPenaltyLastNVal=

# --dry-sequence-breaker STRING - add sequence breaker for DRY sampling
EmbedDrySequenceBreakerCmd=--dry-sequence-breaker
EmbedDrySequenceBreakerVal=

# --dynatemp-range N - dynamic temperature range (default: 0.0, 0.0 = disabled)
EmbedDynatempRangeCmd=--dynatemp-range
EmbedDynatempRangeVal=

# --dynatemp-exp N - dynamic temperature exponent (default: 1.0)
EmbedDynatempExpCmd=--dynatemp-exp
EmbedDynatempExpVal=

# --mirostat N - use Mirostat sampling (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)
EmbedMirostatCmd=--mirostat
EmbedMirostatVal=

# --mirostat-lr N - Mirostat learning rate, parameter eta (default: 0.1)
EmbedMirostatLrCmd=--mirostat-lr
EmbedMirostatLrVal=

# --mirostat-ent N - Mirostat target entropy, parameter tau (default: 5.0)
EmbedMirostatEntCmd=--mirostat-ent
EmbedMirostatEntVal=

# -l, --logit-bias TOKEN_ID(+/-)BIAS - modifies the likelihood of token appearing in the completion
EmbedLogitBiasCmd=--logit-bias
EmbedLogitBiasVal=

# --grammar GRAMMAR - BNF-like grammar to constrain generations (default: '')
EmbedGrammarCmd=--grammar
EmbedGrammarVal=

# --grammar-file FNAME - file to read grammar from
EmbedGrammarFileCmd=--grammar-file
EmbedGrammarFileVal=

# -j, --json-schema SCHEMA - JSON schema to constrain generations
EmbedJsonSchemaCmd=--json-schema
EmbedJsonSchemaVal=

# -jf, --json-schema-file FILE - File containing a JSON schema to constrain generations
EmbedJsonSchemaFileCmd=--json-schema-file
EmbedJsonSchemaFileVal=

# ----- example-specific params -----

# --no-warmup - skip warming up the model with an empty run
EmbedNoWarmupCmd=--no-warmup
EmbedNoWarmupCmdEnabled=false

# --pooling {none,mean,cls,last,rank} - pooling type for embeddings, use model default if unspecified
EmbedPoolingCmd=--pooling
EmbedPoolingVal=mean

# --attention {causal,non-causal} - attention type for embeddings, use model default if unspecified
EmbedAttentionCmd=--attention
EmbedAttentionVal=

# --embd-normalize N - normalisation for embeddings (default: 2) (-1=none, 0=max absolute int16, 1=taxicab, 2=euclidean, >2=p-norm)
EmbedNormalizeCmd=--embd-normalize
EmbedNormalizeVal=1

# --embd-output-format FORMAT - empty = default, "array" = [[],[]...], "json" = openai style, "json+" = same "json" + cosine similarity matrix
EmbedOutputFormatCmd=--embd-output-format
EmbedOutputFormatVal=array

# --embd-separator STRING - separator of embeddings (default \n) for example "<#sep#>"
EmbedSeparatorCmd=--embd-separator
EmbedSeparatorVal=\n

# --embd-bge-small-en-default - use default bge-small-en-v1.5 model
EmbedBgeSmallEnDefaultCmd=--embd-bge-small-en-default
EmbedBgeSmallEnDefaultCmdEnabled=false

# --embd-e5-small-en-default - use default e5-small-v2 model
EmbedE5SmallEnDefaultCmd=--embd-e5-small-en-default
EmbedE5SmallEnDefaultCmdEnabled=false

# --embd-gte-small-default - use default gte-small model
EmbedGteSmallDefaultCmd=--embd-gte-small-default
EmbedGteSmallDefaultCmdEnabled=false


##################################################################################
### Default llama-cli settings - Reordered to match help output ###
Description=Default

# --verbose-prompt - print a verbose prompt before generation (default: false)
VerbosePromptCmd=--verbose-prompt
VerbosePromptCmdEnabled=false

# -t, --threads N - number of threads to use during generation (default: -1)
ThreadsCmd=--threads
ThreadsVal=8

# -tb, --threads-batch N - number of threads to use during batch and prompt processing (default: same as --threads)
ThreadsBatchCmd=--threads-batch
ThreadsBatchVal=8

# -C, --cpu-mask M - CPU affinity mask: arbitrarily long hex. Complements cpu-range (default: "")
CpuMaskCmd=--cpu-mask
CpuMaskVal=

# -Cr, --cpu-range lo-hi - range of CPUs for affinity. Complements --cpu-mask
CpuRangeCmd=--cpu-range
CpuRangeVal=

# --cpu-strict <0|1> - use strict CPU placement (default: 0)
CpuStrictCmd=--cpu-strict
CpuStrictVal=

# --prio N - set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime (default: 0)
PrioCmd=--prio
PrioVal=

# --poll <0...100> - use polling level to wait for work (0 - no polling, default: 50)
PollCmd=--poll
PollVal=

# -Cb, --cpu-mask-batch M - CPU affinity mask: arbitrarily long hex. Complements cpu-range-batch (default: same as --cpu-mask)
CpuMaskBatchCmd=--cpu-mask-batch
CpuMaskBatchVal=

# -Crb, --cpu-range-batch lo-hi - ranges of CPUs for affinity. Complements --cpu-mask-batch
CpuRangeBatchCmd=--cpu-range-batch
CpuRangeBatchVal=

# --cpu-strict-batch <0|1> - use strict CPU placement (default: same as --cpu-strict)
CpuStrictBatchCmd=--cpu-strict-batch
CpuStrictBatchVal=

# --prio-batch N - set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime (default: 0)
PrioBatchCmd=--prio-batch
PrioBatchVal=

# --poll-batch <0|1> - use polling to wait for work (default: same as --poll)
PollBatchCmd=--poll-batch
PollBatchVal=

# -c, --ctx-size N - size of the prompt context (default: 4096, 0 = loaded from model)
CtxSizeCmd=--ctx-size
CtxSizeVal=40960

# -n, --predict, --n-predict N - number of tokens to predict (default: -1, -1 = infinity, -2 = until context filled)
PredictCmd=--n-predict
PredictVal=2560

# -b, --batch-size N - logical maximum batch size (default: 2048)
BatchCmd=--batch-size
BatchCmdVal=2048

# -ub, --ubatch-size N - physical maximum batch size (default: 512)
UBatchCmd=--ubatch-size
UBatchCmdVal=512

# --keep N - number of tokens to keep from the initial prompt (default: 0, -1 = all)
KeepCmd=--keep
KeepVal=-1

# -fa, --flash-attn - enable Flash Attention (default: disabled)
FlashAttentionCmd=--flash-attn
FlashAttentionCmdEnabled=true

# -p, --prompt PROMPT - prompt to start generation with; for system message, use -sys
PromptCmd=--prompt
PromptCmdEnabled=true

# --no-perf - disable internal libllama performance timings (default: false)
NoPerfCmd=--no-perf
NoPerfCmdEnabled=false

# -f, --file FNAME - a file containing the prompt (default: none)
PromptFileCmd=--file
PromptFileVal=

# -bf, --binary-file FNAME - binary file containing the prompt (default: none)
BinaryFileCmd=--binary-file
BinaryFileVal=

# -e, --escape - process escapes sequences (\n, \r, \t, \', \", \\) (default: true)
EscapeNewLinesCmd=-e
EscapeNewLinesCmdEnabled=true

# --no-escape - do not process escape sequences
NoEscapeCmd=--no-escape
NoEscapeCmdEnabled=false

# --rope-scaling {none,linear,yarn} - RoPE frequency scaling method, defaults to linear unless specified by the model
RopeScalingCmd=--rope-scaling
RopeScalingVal=

# --rope-scale N - RoPE context scaling factor, expands context by a factor of N
RopeScaleCmd=--rope-scale
RopeScaleVal=

# --rope-freq-base N - RoPE base frequency, used by NTK-aware scaling (default: loaded from model)
RopeFreqBaseCmd=--rope-freq-base
RopeFreqBaseVal=

# --rope-freq-scale N - RoPE frequency scaling factor, expands context by a factor of 1/N
RopeFreqScaleCmd=--rope-freq-scale
RopeFreqScaleVal=

# --yarn-orig-ctx N - YaRN: original context size of model (default: 0 = model training context size)
YarnOrigContextCmd=--yarn-orig-ctx
YarnOrigContextCmdVal=

# --yarn-ext-factor N - YaRN: extrapolation mix factor (default: -1.0, 0.0 = full interpolation)
YarnExtFactorCmd=--yarn-ext-factor
YarnExtFactorVal=

# --yarn-attn-factor N - YaRN: scale sqrt(t) or attention magnitude (default: 1.0)
YarnAttnFactorCmd=--yarn-attn-factor
YarnAttnFactorVal=

# --yarn-beta-slow N - YaRN: high correction dim or alpha (default: 1.0)
YarnBetaSlowCmd=--yarn-beta-slow
YarnBetaSlowVal=

# --yarn-beta-fast N - YaRN: low correction dim or beta (default: 32.0)
YarnBetaFastCmd=--yarn-beta-fast
YarnBetaFastVal=

# -dkvc, --dump-kv-cache - verbose print of the KV cache
DumpKvCacheCmd=--dump-kv-cache
DumpKvCacheCmdEnabled=false

# -nkvo, --no-kv-offload - disable KV offload
NoKvOffloadCmd=--no-kv-offload
NoKvOffloadCmdEnabled=false

# -ctk, --cache-type-k TYPE - KV cache data type for K (default: f16)
CacheTypeKCmd=--cache-type-k
CacheTypeKVal=

# -ctv, --cache-type-v TYPE - KV cache data type for V (default: f16)
CacheTypeVCmd=--cache-type-v
CacheTypeVVal=

# -dt, --defrag-thold N - KV cache defragmentation threshold (default: 0.1, < 0 - disabled)
DefragTholdCmd=--defrag-thold
DefragTholdVal=

# -np, --parallel N - number of parallel sequences to decode (default: 1)
ParallelCmd=--parallel
ParallelVal=

# --rpc SERVERS - comma separated list of RPC servers
RpcCmd=--rpc
RpcVal=

# --mlock - force system to keep model in RAM rather than swapping or compressing
MemLockCmd=--mlock
MemLockCmdEnabled=false

# --no-mmap - do not memory-map model (slower load but may reduce pageouts if not using mlock)
NoMmapCmd=--no-mmap
NoMmapCmdEnabled=false

# --numa TYPE - attempt optimizations that help on some NUMA systems
NumaCmd=--numa
NumaVal=

# --override-tensor, -ot <tensor name pattern>=<buffer type>,... - override tensor buffer type
OverrideTensorCmd=--override-tensor
OverrideTensorVal=

# -ngl, --gpu-layers, --n-gpu-layers N - number of layers to store in VRAM
GPULayersCmd=--n-gpu-layers
GPULayersVal=33

# -sm, --split-mode {none,layer,row} - how to split the model across multiple GPUs
SplitModeCmd=--split-mode
SplitModeCmdVal=row

# -ts, --tensor-split N0,N1,N2,... - fraction of the model to offload to each GPU
TensorSplitCmd=--tensor-split
TensorSplitVal=

# -mg, --main-gpu INDEX - the GPU to use for the model (default: 0)
MainGPUCmd=--main-gpu
MainGPUVal=2

# --check-tensors - check model tensor data for invalid values (default: false)
CheckTensorsCmd=--check-tensors
CheckTensorsCmdEnabled=false

# --override-kv KEY=TYPE:VALUE - advanced option to override model metadata by key
OverrideKvCmd=--override-kv
OverrideKvVal=

# --lora FNAME - path to LoRA adapter (can be repeated to use multiple adapters)
LoraCmd=--lora
LoraVal=

# --lora-scaled FNAME SCALE - path to LoRA adapter with user defined scaling
LoraScaledCmd=--lora-scaled
LoraScaledVal=

# --control-vector FNAME - add a control vector
ControlVectorCmd=--control-vector
ControlVectorVal=

# --control-vector-scaled FNAME SCALE - add a control vector with user defined scaling SCALE
ControlVectorScaledCmd=--control-vector-scaled
ControlVectorScaledVal=

# --control-vector-layer-range START END - layer range to apply the control vector(s) to
ControlVectorLayerRangeCmd=--control-vector-layer-range
ControlVectorLayerRangeVal=

# -m, --model FNAME - model path
ModelCmd=--model
ModelFullPathVal=C:/Projects/byte-vision/models/Qwen3-8B-Q8_0.gguf

# -mu, --model-url MODEL_URL - model download url (default: unused)
ModelUrlCmd=--model-url
ModelUrlVal=

# -hf, -hfr, --hf-repo <user>/<model>[:quant] - Hugging Face model repository
HfRepoCmd=--hf-repo
HfRepoVal=

# -hfd, -hfrd, --hf-repo-draft <user>/<model>[:quant] - Same as --hf-repo, but for the draft model
HfRepoDraftCmd=--hf-repo-draft
HfRepoDraftVal=

# -hff, --hf-file FILE - Hugging Face model file
HfFileCmd=--hf-file
HfFileVal=

# -hfv, -hfrv, --hf-repo-v <user>/<model>[:quant] - Hugging Face model repository for the vocoder model
HfRepoVCmd=--hf-repo-v
HfRepoVVal=

# -hffv, --hf-file-v FILE - Hugging Face model file for the vocoder model
HfFileVCmd=--hf-file-v
HfFileVVal=

# -hft, --hf-token TOKEN - Hugging Face access token
HfTokenCmd=--hf-token
HfTokenVal=

# --log-disable - Log disable
LogDisableCmd=--log-disable
LogDisableCmdEnabled=true

# --log-file FNAME - Log to file
ModelLogFileCmd=--log-file
ModelLogFileNameVal=C:/Projects/byte-vision/logs/Qwen3-8B-Q8_0.log


# --log-colors - Enable colored logging
LogColorsCmd=--log-colors
LogColorsCmdEnabled=false

# -v, --verbose, --log-verbose - Set verbosity level to infinity
LogVerboseCmd=--log-verbose
LogVerboseEnabled=false

# -lv, --verbosity, --log-verbosity N - Set the verbosity threshold
LogVerbosityCmd=--log-verbosity
LogVerbosityVal=

# --log-prefix - Enable prefix in log messages
LogPrefixCmd=--log-prefix
LogPrefixCmdEnabled=false

# --log-timestamps - Enable timestamps in log messages
LogTimestampsCmd=--log-timestamps
LogTimestampsCmdEnabled=false

# ----- sampling params -----

# --samplers SAMPLERS - samplers that will be used for generation in the order
SamplersCmd=--samplers
SamplersVal=

# -s, --seed SEED - RNG seed (default: -1, use random seed for -1)
RandomSeedCmd=--seed
RandomSeedVal=112358

# --sampling-seq, --sampler-seq SEQUENCE - simplified sequence for samplers that will be used
SamplingSeqCmd=--sampling-seq
SamplingSeqVal=

# --ignore-eos - ignore end of stream token and continue generating
IgnoreEosCmd=--ignore-eos
IgnoreEosCmdEnabled=false

# --temp N - temperature (default: 0.8)
TemperatureCmd=--temp
TemperatureVal=0.6

# --top-k N - top-k sampling (default: 40, 0 = disabled)
TopKCmd=--top-k
TopKVal=20

# --top-p N - top-p sampling (default: 0.9, 1.0 = disabled)
TopPCmd=--top-p
TopPVal=0.95

# --min-p N - min-p sampling (default: 0.1, 0.0 = disabled)
MinPCmd=--min-p
MinPVal=0

# --xtc-probability N - xtc probability (default: 0.0, 0.0 = disabled)
XtcProbabilityCmd=--xtc-probability
XtcProbabilityVal=

# --xtc-threshold N - xtc threshold (default: 0.1, 1.0 = disabled)
XtcThresholdCmd=--xtc-threshold
XtcThresholdVal=

# --typical N - locally typical sampling, parameter p (default: 1.0, 1.0 = disabled)
TypicalCmd=--typical
TypicalVal=

# --repeat-last-n N - last n tokens to consider for penalize (default: 64, 0 = disabled, -1 = ctx_size)
RepeatLastPenaltyCmd=--repeat-last-n
RepeatLastPenaltyVal=

# --repeat-penalty N - penalize repeat sequence of tokens (default: 1.0, 1.0 = disabled)
RepeatPenaltyCmd=--repeat-penalty
RepeatPenaltyVal=

# --presence-penalty N - repeat alpha presence penalty (default: 0.0, 0.0 = disabled)
PresencePenaltyCmd=--presence-penalty
PresencePenaltyVal=

# --frequency-penalty N - repeat alpha frequency penalty (default: 0.0, 0.0 = disabled)
FrequencyPenaltyCmd=--frequency-penalty
FrequencyPenaltyVal=

# --dry-multiplier N - set DRY sampling multiplier (default: 0.0, 0.0 = disabled)
DryMultiplierCmd=--dry-multiplier
DryMultiplierVal=

# --dry-base N - set DRY sampling base value (default: 1.75)
DryBaseCmd=--dry-base
DryBaseVal=

# --dry-allowed-length N - set allowed length for DRY sampling (default: 2)
DryAllowedLengthCmd=--dry-allowed-length
DryAllowedLengthVal=

# --dry-penalty-last-n N - set DRY penalty for the last n tokens (default: -1, 0 = disable, -1 = context size)
DryPenaltyLastNCmd=--dry-penalty-last-n
DryPenaltyLastNVal=

# --dry-sequence-breaker STRING - add sequence breaker for DRY sampling
DrySequenceBreakerCmd=--dry-sequence-breaker
DrySequenceBreakerVal=

# --dynatemp-range N - dynamic temperature range (default: 0.0, 0.0 = disabled)
DynatempRangeCmd=--dynatemp-range
DynatempRangeVal=

# --dynatemp-exp N - dynamic temperature exponent (default: 1.0)
DynatempExpCmd=--dynatemp-exp
DynatempExpVal=

# --mirostat N - use Mirostat sampling (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)
MirostatCmd=--mirostat
MirostatVal=

# --mirostat-lr N - Mirostat learning rate, parameter eta (default: 0.1)
MirostatLrCmd=--mirostat-lr
MirostatLrVal=

# --mirostat-ent N - Mirostat target entropy, parameter tau (default: 5.0)
MirostatEntCmd=--mirostat-ent
MirostatEntVal=

# -l, --logit-bias TOKEN_ID(+/-)BIAS - modifies the likelihood of token appearing in the completion
LogitBiasCmd=--logit-bias
LogitBiasVal=

# --grammar GRAMMAR - BNF-like grammar to constrain generations (default: '')
GrammarCmd=--grammar
GrammarVal=

# --grammar-file FNAME - file to read grammar from
GrammarFileCmd=--grammar-file
GrammarFileVal=

# -j, --json-schema SCHEMA - JSON schema to constrain generations
JsonSchemaCmd=--json-schema
JsonSchemaVal=

# -jf, --json-schema-file FILE - File containing a JSON schema to constrain generations
JsonSchemaFileCmd=--json-schema-file
JsonSchemaFileVal=

# --no-display-prompt - don't print prompt at generation (default: false)
NoDisplayPromptCmd=--no-display-prompt
NoDisplayPromptEnabled=true

# -co, --color - colorise output to distinguish prompt and user input from generations (default: false)
ColorCmd=--color
ColorCmdEnabled=false

# --no-context-shift - disables context shift on infinite text generation (default: disabled)
NoContextShiftCmd=--no-context-shift
NoContextShiftCmdEnabled=false

# -sys, --system-prompt PROMPT - system prompt to use with model (if applicable, depending on chat template)
SystemPromptCmd=--system-prompt
SystemPromptVal=

# -sysf, --system-prompt-file FNAME - a file containing the system prompt (default: none)
SystemPromptFileCmd=--system-prompt-file
SystemPromptFileVal=

# -ptc, --print-token-count N - print token count every N tokens (default: -1)
PrintTokenCountCmd=--print-token-count
PrintTokenCountVal=-1

# --prompt-cache FNAME - file to cache prompt state for faster startup (default: none)
PromptCacheCmd=--prompt-cache
PromptCacheVal=C:/Projects/byte-vision/prompt-cache/Qwen3-8B-Q8_0

# --prompt-cache-all - if specified, saves user input and generations to cache as well
PromptCacheAllCmd=--prompt-cache-all
PromptCacheAllCmdEnabled=true

# --prompt-cache-ro - if specified, uses the prompt cache but does not update it
PromptCacheRoCmd=--prompt-cache-ro
PromptCacheRoCmdEnabled=false

# -r, --reverse-prompt PROMPT - halt generation at PROMPT, return control in interactive mode
ReversePromptCmd=--reverse-prompt
ReversePromptVal=

# -sp, --special - special tokens output enabled (default: false)
SpecialCmd=--special
SpecialCmdEnabled=false

# -cnv, --conversation - run in conversation mode (default: auto enabled if chat template is available)
ConversationCmd=--conversation
ConversationCmdEnabled=false

# -no-cnv, --no-conversation - force disable conversation mode (default: false)
NoConversationCmd=--no-conversation
NoConversationCmdEnabled=true

# -st, --single-turn - run conversation for a single turn only, then exit when done (default: false)
SingleTurnCmd=--single-turn
SingleTurnCmdEnabled=false

# -i, --interactive - run in interactive mode (default: false)
InteractiveCmd=--interactive
InteractiveCmdEnabled=false

# -if, --interactive-first - run in interactive mode and wait for input right away (default: false)
InteractiveFirstCmd=--interactive-first
InteractiveFirstCmdEnabled=false

# -mli, --multiline-input - allows you to write or paste multiple lines without ending each in '\'
MultilineInputCmd=--multiline-input
MultilineInputCmdEnabled=true

# --in-prefix-bos - prefix BOS to user inputs, preceding the `--in-prefix` string
InPrefixBosCmd=--in-prefix-bos
InPrefixBosCmdEnabled=false

# --in-prefix STRING - string to prefix user inputs with (default: empty)
InPrefixCmd=--in-prefix
InPrefixVal=

# --in-suffix STRING - string to suffix after user inputs with (default: empty)
InSuffixCmd=--in-suffix
InSuffixVal=

# --no-warmup - skip warming up the model with an empty run
NoWarmupCmd=--no-warmup
NoWarmupCmdEnabled=false

# -gan, --grp-attn-n N - group-attention factor (default: 1)
GrpAttnNCmd=--grp-attn-n
GrpAttnNVal=1

# -gaw, --grp-attn-w N - group-attention width (default: 512)
GrpAttnWCmd=--grp-attn-w
GrpAttnWVal=512

# --jinja - use jinja template for chat (default: disabled)
JinjaCmd=--jinja
JinjaCmdEnabled=false

# --reasoning-format FORMAT - reasoning format (default: deepseek; allowed values: deepseek, none)
ReasoningFormatCmd=--reasoning-format
ReasoningFormatVal=none

# --chat-template JINJA_TEMPLATE - set custom jinja chat template
ChatTemplateCmd=--chat-template
ChatTemplateVal=

# --chat-template-file JINJA_TEMPLATE_FILE - set custom jinja chat template file
ChatTemplateFileCmd=--chat-template-file
ChatTemplateFileVal=

# --simple-io - use basic IO for better compatibility in subprocesses and limited consoles
SimpleIoCmd=--simple-io
SimpleIoCmdEnabled=false
